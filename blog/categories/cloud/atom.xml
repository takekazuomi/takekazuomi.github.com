<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Cloud | Cloud Memo]]></title>
  <link href="http://takekazuomi.github.com/blog/categories/cloud/atom.xml" rel="self"/>
  <link href="http://takekazuomi.github.com/"/>
  <updated>2012-12-28T21:19:08+09:00</updated>
  <id>http://takekazuomi.github.com/</id>
  <author>
    <name><![CDATA[Takekazu Omi]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Azure Storage Client 2.0 CompletedSynchronously FIX]]></title>
    <link href="http://takekazuomi.github.com/blog/2012/12/26/asc2-dot-0asyncbug/"/>
    <updated>2012-12-26T23:59:00+09:00</updated>
    <id>http://takekazuomi.github.com/blog/2012/12/26/asc2-dot-0asyncbug</id>
    <content type="html"><![CDATA[<p>以前の記事<a href="/blog/2012/12/08/WAAC2012Day2/">Azure Storage Gen 2は速かった</a>の補足です。その<a href="/blog/2012/12/08/WAAC2012Day2/#pending">記事</a>で、</p>

<blockquote><p>このコードを動かしてみたら、「単一スレッド＋非同期の組み合わせだと、おおよそ２から３程度のコネクションしか作成されない」ことに気が付きました。場合によっては、5ぐらいまで上がることもあるようですが、どうしてこうなるのか不思議です。
<strong> これは、Azure Storage Client 2.0のBUG </strong> だったようです。2.0.2で修正されています。</p></blockquote>

<p>と書きました、結局執筆時点でのAzure Storage Client 2.0.1にはBUGがあり、後日2.0.2で修正されたことが分かったのですが確認したことも含めまとめてみました。</p>

<p><a href="http://www.flickr.com/photos/takekazuomi/8307052288/" title="candle by takekazu, on Flickr"><img src="http://farm9.staticflickr.com/8082/8307052288_2a8cdf5678_c.jpg" width="800" height="600" alt="candle"></a></p>

<hr />

<h1>BUGの内容</h1>

<p>BUGの内容としては、非同期メソッドが返すIAsyncResultオブジェクトのCompletedSynchronouslyプロパティが一貫性の無い値になっていて、その結果、TaskFactory.FromAsyncが正しく動作しないというものでした。</p>

<ul>
<li>参照</li>
<li><p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/changelog.txt">WindowsAzure/azure-sdk-for-net changelog.txt</a></p></li>
<li><p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/pull/134">WindowsAzure/azure-sdk-for-net Issue #141:</a></p></li>
</ul>


<hr />

<h1>再現試験</h1>

<p>まずは、2.0.1での問題の再現性の確認し、2.0.3で解決されているのかを検証します。コードは<a href="/blog/2012/12/08/WAAC2012Day2/">前の記事</a>とほとんど同じですが、なるべく簡略化したものにしています。</p>

<p>まずは、APM (Asynchronous Programming Model)パターンの非同期メソッドをTask.FromAsync()でラップしてExecuteAsyncメソッドを作ります。今回問題となっているのは、CloudTable.BeginExecute から、AsyncCallback を呼び出すときに渡すIAsyncResultオブジェクトのCompletedSynchronouslyプロパティです。ちょと問題があるような気がしますが、今回はこれで行きます。</p>

<p><div><script src='https://gist.github.com/4369349.js?file='></script>
<noscript><pre><code>public static class CloudTableExtensions
{
    public static Task&lt;TableResult&gt; ExecuteAsync(this CloudTable cloudTable, TableOperation operation, TableRequestOptions requestOptions = null, OperationContext operationContext = null, object state = null)
    {
        return Task.Factory.FromAsync&lt;TableOperation, TableRequestOptions, OperationContext, TableResult&gt;(
        cloudTable.BeginExecute, cloudTable.EndExecute, operation, requestOptions, operationContext, state);
    }
}
</code></pre></noscript></div>
</p>

<p>このExecuteAsyncを使って指定回ループしてテーブルにエンティティをInsertします。</p>

<p><div><script src='https://gist.github.com/4369329.js?file='></script>
<noscript><pre><code>using System;
using System.Linq;
using System.Threading.Tasks;
using Microsoft.WindowsAzure.Storage;
using Microsoft.WindowsAzure.Storage.Table;

namespace AsyncSemaphore
{
    partial class Program
    {
        static void InsertAsync001(CloudTable table, int start, int count)
        {
            var tasks = Enumerable.Range(start, count).Select(async i =&gt;
            {
                var e = new TableEntity(PKEY_STRS[i % PKEY_STRS.Length], i.ToString(&quot;D10&quot;));

                var insertOperation = TableOperation.Insert(e);
                try
                {
                    await table.ExecuteAsync(insertOperation).ConfigureAwait(false);

                    if (i != 0 &amp;&amp; i % 100000 == 0)
                        Console.Error.WriteLine(i);
                }
                catch (StorageException se)
                {
                    Console.Error.WriteLine(&quot;{0}, {1}&quot;, i, se.ToString());
                }
            }).ToArray();

            Task.WhenAll(tasks);
        }
    }
}
</code></pre></noscript></div>
</p>

<p>このコードは、Insertの数だけ、Taskが生成されて全部まとめてWaitしています。これを、.NET 4.0でやるとTask毎にWait Handleを確保するので非常に効率が悪いですが、.NET 4.5では、Waitの数しかリソースを使わないので、そんなに悪くありません。それでも件数に応じて使用メモリーが増えるので本番で使うのはあまりお勧めできないコーディングパターンです。</p>

<p>.NET 4.5のTask回りの変更については、このBlogの記事「<a href="http://csharptan.wordpress.com/2011/12/11/%E6%96%B0%E6%A9%9F%E8%83%BD%E3%81%8C%E5%85%A5%E3%82%8B%E3%81%BE%E3%81%A7/">C#たんっ！ 新機能が入るまで</a>」から読み始めるのがお勧めです、必要な部分へのリンクが張られています。</p>

<h2>2.0.1 で動かす</h2>

<p>このコードを、Azure Storage Client 2.0.1 で動かしてみます。ライブラリのバージョンを指定するには、nugetを使うと便利です。もし、すでにAzure Storage Client が入っていたら下記のように削除してからバージョンを指定して入れ直します。</p>

<p>```</p>

<blockquote><p>Uninstall-Package WindowsAzure.Storage –RemoveDependencies</p>

<p>Install-Package  WindowsAzure.Storage -Version 2.0.1</p></blockquote>

<p>```</p>

<p>これで動かします。非同期メソッドが本当に非同期で動いているかどうかの確認はUIならUI Threadがブロックされていているかどうかなどで分かり易いのですが、サーバーサイドのプログラム（今回コンソールですが）ではわかり辛い面があります。このコードはAzure Storageとの間でSocketを張っているのでTCP/IP接続の数を見ることで並列度が分かります。あとは、ネットワーク転送速度（Send）も参考になります。</p>

<ul>
<li>Azure Storage Client 2.0.1 時のResource Moniter画面
<img src="/images/2012_12/connections-asc2.0.1.png" alt="2.0.1時のResource Moniter画面" /></li>
</ul>


<p>見事に接続数が伸びません。</p>

<h2>2.0.3では？</h2>

<p>これを、2.0.3 でビルドし直します。2012/12/24現在の最新が2.0.3でバージョン指定しないと最新版が落ちてきます。</p>

<p>```</p>

<blockquote><p>Uninstall-Package WindowsAzure.Storage –RemoveDependencies</p>

<p>Install-Package  WindowsAzure.Storage</p></blockquote>

<p>```</p>

<ul>
<li>Azure Storage Client 2.0.3 時のResource Moniter画面
<img src="/images/2012_12/connections-asc2.0.3.png" alt="2.0.3時のResource Moniter画面" /></li>
</ul>


<h2>結論</h2>

<p>劇的にコネクション数が変わります。画面だとコネクションの数ははっきりとわかりませんが、 2.0.1 の時の画面と全く違っているのがわかると思います。数を数えると開始直後に1000接続以上が作成されます。これで、2.0.1の実装には問題があり、非同期メソッドを使ってもほとんど非同期に実行されてなかったこと、それが、2.0.3では修正されていることが確認できました。</p>

<p>ちなみに、今回確認はしていませんが、以前に1.4のAzure Storage Clientを試した時には非同期メソッドで同時接続数が少なくて困るという問題は無ありませんでした、2.0で発生したBUGで2.0.2でFIXということのようです。</p>

<hr />

<h1>次の問題</h1>

<p>万事解決、良かった良かったと言いたいところですが別の問題が起きます。並列度があがったのは良いのですが、コネクションを張りすぎてExceptionが大量に発生します。</p>

<ul>
<li>Azure Storage Client 2.0.3 時でのException
<img src="/images/2012_12/connections-asc2.0.3-cmd.png" alt="Azure Storage Client 2.0.3 時でのException" /></li>
</ul>


<p>何らかの方法で、並列度を制限しないと実用的ではありません。特にバッチの中で非同期呼び出しを使う場合などはこれは致命的です。</p>

<p>ここでは、Blob でのUpload処理が参考になります。
<a href="/blog/2012/12/08/blobasyncinside/#borderofsyncasync">Windows Azure Storage 2.0 の Blob Upload の同期と非同期の境界</a>で参照している処理を見ると、Semaphoreを使って非同期処理には入れる数を制御しているのがわかる。</p>

<h2>Semaphoreを使う</h2>

<p>上記の処理方法に習って、Semaphoreを使って同時実行数を制御する。SemaphoreSlim という便利がものがあるのでそれを使うことにします。
こうすることで、同時実行数を制御することがでます。とりあえず100で制限します。これで普通に動きます。</p>

<p><div><script src='https://gist.github.com/4369880.js?file='></script>
<noscript><pre><code>static void InsertAsync002(CloudTable table, int start, int count)
{
    var throttler = new SemaphoreSlim(initialCount: 100);

    var tasks = Enumerable.Range(start, count).Select(async i =&gt;
    {

        throttler.Wait();

        var e = new TableEntity(PKEY_STRS[i % PKEY_STRS.Length], i.ToString(&quot;D10&quot;));

        var insertOperation = TableOperation.Insert(e);
        try
        {

            await table.ExecuteAsync(insertOperation).ConfigureAwait(false);

            if (i != 0 &amp;&amp; i % 100000 == 0)
                Console.Error.WriteLine(i);

        }
        catch (StorageException se)
        {
            Console.Error.WriteLine(&quot;{0}, {1}&quot;, i, se.ToString());
        }
        finally
        {
            throttler.Release();
        }
    }).ToArray();

    Task.WhenAll(tasks);
}
</code></pre></noscript></div>
</p>

<hr />

<h1>まとめ</h1>

<ul>
<li>Azure Storage Client 2.0 は、2.0.2で非同期周りのBUGが直っている。</li>
<li>非同期呼び出しをループ内で使うと過剰にリソースを消費することがある。</li>
<li>同時実行数を制御するにはSemaphoreを使うと制限できる。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Azure Virtual MachineのDISK性能]]></title>
    <link href="http://takekazuomi.github.com/blog/2012/12/23/azurevmfix1221/"/>
    <updated>2012-12-23T18:40:00+09:00</updated>
    <id>http://takekazuomi.github.com/blog/2012/12/23/azurevmfix1221</id>
    <content type="html"><![CDATA[<p>twitterで、「<a href="https://twitter.com/kamebuchi/status/282094138269261825">Azure VMのLinuxを21日以降作るか、更新手順を実施するとパフォーマンスが改善されるらしー</a>」というのを読んで、以前DISK性能を調べ始めてそのまま放置していたのを思い出した。<a href="http://www.slideshare.net/takekazuomi/azure-ubuntu-1204-iozone" title="Azure Ubuntu 12.04 iozone 速報 2012/7/4">Azure Ubuntu 12.04 iozone 速報 2012/7/4</a></p>

<p>Azure Storageの非同期と同期の比較をしようと始めたのだけど、なかなか手間取って進まない。ちょっと寄り道して速くなったというAzure VMを試してみることにした。</p>

<p>前のVMは消してしまったので、新たにインストールし直すところから始める。AzureのポータルからUbuntuをインストールして、DataDiskを接続するあたりまでは他に任せてubuntuが起動した後から書いていきます。</p>

<p><a href="http://www.flickr.com/photos/takekazuomi/7987640250/" title="Triangle by takekazu, on Flickr"><img src="http://farm9.staticflickr.com/8450/7987640250_b87fdcf1f1_c.jpg" width="800" height="534" alt="Triangle"></a></p>

<hr />

<h1>Ubuntu 環境の準備</h1>

<p>基本的には、前回と同じになるようにします。だたUbuntuを12.10にして、data diskのホストキャッシュの設定を変えて3つのDISKを接続して測定しました。<a href="http://www.slideshare.net/takekazuomi/azure-ubuntu-1204-iozone" title="Azure Ubuntu 12.04 iozone 速報 2012/7/4">以前の測定</a>の時は、ホストキャッシュの設定はポータルからはできずに、デフォルトでした。
その時(2012/7/4)は、ホストキャッシュ無しがデフォルトだったと思うのですが、ちょっとドキュメントが見つからないので前の結果は参考程度にしてください。</p>

<p>Azure iDC は、West USで、2 coreのインスタンス（M）を使いました。Sにするか少し考えたのですが、クラウドサービスについては、I/O パフォーマンスがXS、Sでは制限されている、参考：<a href="http://www.windowsazure.com/ja-jp/pricing/details/">Windows Azure の料金と、請求の計測単位の詳細</a>ようなのでMを使うことにしました。</p>

<p>正確には、今回試そうとしているVirtual Machine はまだ Previewで Cloud Serviceと同じような制限になるかは情報が公開されていない（私は知らないだけかもしれませんが）のですが、同じになってそうな気がしたのでMにしました。</p>

<p>ちょっと古いものでは、<a href="http://msdn.microsoft.com/ja-jp/library/windowsazure/ee814754.aspx">仮想マシンのサイズの構成方法</a> という情報もあります。</p>

<p>インスタンスの選択で考慮する必要があると思われるのは、Data Diskはネットワーク経由で接続される、ソフトウェアで処理する部分が多い＝CPUを使うということです。従ってネットワーク帯域制限やCore数の影響を無視できないはずです。XSやSのインスタンスだと何を測定しているのか不安になる気がしたのでMを選択しました。
実際どのインスタンスサイズの程度影響があるのかは興味ありますが未測定です。</p>

<p>ざっと流すと、以下のような手順踏んで用意をします。</p>

<ol>
<li>Ubuntu 12.10 を、azure portalから、virtual machineイメージをインストール</li>
<li>data disk を、256Gで3つ作成、/dev/sdc, sdd, sdeを確認、キャッシュをそれぞれ「なし、読み取り専用、読み取り/書き込み」と指定</li>
<li>fdiskして、/dev/sd[cde]1にext4でfilesystemを作成し/mnt/data, /mnt/data1, /mnt/data2へmount</li>
<li>apt-get update, upgrade して最新に更新</li>
<li>/etc/apt/sources.list で、multiverse を追加（コメントを外しただけ）</li>
<li>apt-get install iozone3 でインストール</li>
</ol>


<h3>ディスク構成</h3>

<table width="80%" border="1">
    <thead>
    <tr>
    <th>ディスク</th>
    <th>種類           </th>
    <th>ホスト キャッシュ </th>
    <th>サイズ</th>
    <th>備考</th>
    </tr>
    </thead>
    <tbody>
    <tr>
    <td>/dev/sda</td>
    <td>OS ディスク    </td>
    <td>読み取り/書き込み </td>
    <td>30GB  </td>
    <td></td>
    </tr>
    <tr>
    <td>/dev/sdc</td>
    <td>データ ディスク</td>
    <td>なし              </td>
    <td>256GB </td>
    <td></td>
    </tr>
    <tr>
    <td>/dev/sdd</td>
    <td>データ ディスク</td>
    <td>読み取り専用      </td>
    <td>256GB </td>
    <td></td>
    </tr>
    <tr>
    <td>/dev/sde</td>
    <td>データ ディスク</td>
    <td>読み取り/書き込み </td>
    <td>256GB </td>
    <td></td>
    </tr>
    </tbody>
</table>


<hr />

<h2>コマンドライン</h2>

<p>今後の再テストのためのメモも兼ねて、コマンドをラインに流したもの抜粋を挙げておきます。
(以下sudo省略)</p>

<ul>
<li>ポータルで256Gでdata diskを作成して接続を確認</li>
</ul>


<p><code>
$ dmesg | grep -e "\[sd[a-z]\]"
sd 2:0:0:0: [sda] 62914560 512-byte logical blocks: (32.2 GB/30.0 GiB)
sd 2:0:0:0: [sda] Write Protect is off
sd 2:0:0:0: [sda] Mode Sense: 0f 00 10 00
sd 2:0:0:0: [sda] Write cache: enabled, read cache: enabled, supports DPO and FUA
sd 2:0:0:0: [sda] Attached SCSI disk
sd 3:0:1:0: [sdb] 283115520 512-byte logical blocks: (144 GB/135 GiB)
sd 3:0:1:0: [sdb] Write Protect is off
sd 3:0:1:0: [sdb] Mode Sense: 0f 00 10 00
sd 3:0:1:0: [sdb] Write cache: enabled, read cache: enabled, supports DPO and FUA
sd 3:0:1:0: [sdb] Attached SCSI disk
sd 6:0:0:0: [sdc] 536870912 512-byte logical blocks: (274 GB/256 GiB)
sd 6:0:0:0: [sdc] Write Protect is off
sd 6:0:0:0: [sdc] Mode Sense: 0f 00 10 00
sd 6:0:0:0: [sdc] Write cache: enabled, read cache: enabled, supports DPO and FUA
sd 6:0:0:0: [sdc] Attached SCSI disk
sd 6:0:0:1: [sdd] 536870912 512-byte logical blocks: (274 GB/256 GiB)
sd 6:0:0:1: [sdd] Write Protect is off
sd 6:0:0:1: [sdd] Mode Sense: 0f 00 10 00
sd 6:0:0:1: [sdd] Write cache: enabled, read cache: enabled, supports DPO and FUA
sd 6:0:0:1: [sdd] Attached SCSI disk
sd 6:0:0:2: [sde] 536870912 512-byte logical blocks: (274 GB/256 GiB)
sd 6:0:0:2: [sde] Write Protect is off
sd 6:0:0:2: [sde] Mode Sense: 0f 00 10 00
sd 6:0:0:2: [sde] Write cache: enabled, read cache: enabled, supports DPO and FUA
sd 6:0:0:2: [sde] Attached SCSI disk
</code></p>

<ul>
<li>parted で全セクタを使ってパーテーションを作成</li>
</ul>


<p>```
$ sudo parted /dev/sdc --script mklabel gpt
$ sudo parted /dev/sdc --script 'mkpart disk1 ext4 1M -1'
$ sudo parted /dev/sdc --script 'print'</p>

<p>Model: Msft Virtual Disk (scsi)
Disk /dev/sdc: 275GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt</p>

<p>Number  Start   End    Size   File system  Name   Flags
 1      1049kB  275GB  275GB  ext4         disk1</p>

<p>$ sudo parted /dev/sdd --script mklabel gpt
$ sudo parted /dev/sdd --script 'mkpart disk2 ext4 1M -1'
$ sudo parted /dev/sdd --script 'print'</p>

<p>... snip ...</p>

<p>$ sudo parted /dev/sde --script mklabel gpt
$ sudo parted /dev/sde --script 'mkpart disk3 ext4 1M -1'
$ sudo parted /dev/sde --script 'print'</p>

<p>... snip ...</p>

<p>$ sudo mkfs.ext4 /dev/sdc1
... snip ...
$ sudo mkfs.ext4 /dev/sdd1
... snip ...
$ sudo mkfs.ext4 /dev/sde1
... snip ...
```</p>

<ul>
<li>mount point 作って、/mnt/resouceとともにパーミッションを変更</li>
</ul>


<p>```
$ mkdir /mnt/data1 /mnt/data2 /mnt/data3
$ chmod a+wrx /mnt/data*
$ chmod a+wrx /mnt/resource
$ ls -l /mnt/
total 20
drwx------ 3 root root 4096 Dec 21 15:56 cdrom
drwxrwxrwx 2 root root 4096 Dec 22 21:04 data1
drwxrwxrwx 2 root root 4096 Dec 22 21:04 data2
drwxrwxrwx 2 root root 4096 Dec 22 22:47 data3
drwxrwxrwx 4 root root 4096 Dec 21 22:14 resource</p>

<p>```</p>

<ul>
<li>とりあえず、マウントして確認</li>
</ul>


<p>```
$ sudo mount -t ext4 /dev/sdc1 /mnt/data1
$ sudo mount -t ext4 /dev/sdd1 /mnt/data2
$ sudo mount -t ext4 /dev/sde1 /mnt/data3</p>

<p>$ df -T
Filesystem     Type     1K-blocks    Used Available Use% Mounted on
/dev/sda1      ext4      30953664 1142056  28539332   4% /
udev           devtmpfs   1751196      12   1751184   1% /dev
tmpfs          tmpfs       704872     280    704592   1% /run
none           tmpfs         5120       0      5120   0% /run/lock
none           tmpfs      1762172       0   1762172   0% /run/shm
none           tmpfs       102400       0    102400   0% /run/user
/dev/sdb1      ext4     139334632  192000 132064848   1% /mnt/resource
/dev/sdc1      ext4     264221700  191576 250608456   1% /mnt/data1
/dev/sdd1      ext4     264221700  191576 250608456   1% /mnt/data2
/dev/sde1      ext4     264221700  191576 250608456   1% /mnt/data3
```</p>

<ul>
<li>再起動してもマウントされるように、UUIDを確認して /etc/fstab に追加。</li>
</ul>


<p>```
$ blkid
/dev/sda1: LABEL="cloudimg-rootfs" UUID="56d8a977-c1fe-461e-a328-b19fc47c743f" TYPE="ext4"
/dev/sdb1: UUID="d063d8a2-32fc-486c-a9b4-e6bcf7e5deae" TYPE="ext4"
/dev/sdd1: UUID="88f28b19-fdc6-46dc-a2d7-2daa1754754f" TYPE="ext4"
/dev/sdc1: UUID="a1cb5045-178a-476e-9821-084f8f6d92a6" TYPE="ext4"
/dev/sde1: UUID="15b8b45e-fbd0-4efc-9534-5e38b1877828" TYPE="ext4"</p>

<p>$ vi /etc/fstab</p>

<p>... snip ...</p>

<p>$ cat /etc/fstab
UUID=56d8a977-c1fe-461e-a328-b19fc47c743f       /        ext4   defaults        0 0
UUID=a1cb5045-178a-476e-9821-084f8f6d92a6       /mnt/data1        ext4   defaults        0 0
UUID=88f28b19-fdc6-46dc-a2d7-2daa1754754f       /mnt/data2        ext4   defaults        0 0
UUID=15b8b45e-fbd0-4efc-9534-5e38b1877828       /mnt/data3        ext4   defaults        0 0</p>

<p>```</p>

<ul>
<li>最新にして再起動する</li>
</ul>


<p>```
$ apt-get update</p>

<p>... snip ...</p>

<p>$ apt-get upgrade</p>

<p>... snip ...</p>

<p>$ shutdown -r now</p>

<p>```</p>

<ul>
<li>iozone3 を入れる</li>
</ul>


<p>/etc/apt/sources.list を変更して、multiverse を追加（コメントを外しただけ）</p>

<p>```
$ vi /etc/apt/sources.list</p>

<p>... snip ...</p>

<p>$ apt-get update
$ apt-get install iozone3
```</p>

<hr />

<h1>測定</h1>

<p>これで環境が出来たので測定します。基本的には、iozone 一発で細かいオプションの指定はしません。なんとなく、Excelファイルにしたのですが、面倒になるだけであまりメリットは無かったかもしれません。もし再試験してもらえるなら</p>

<p><code>
iozone -Ra -f /mnt/resource/tmp/test -b sdb2-001.xls -s 1g
iozone -Ra -f /mnt/data1/tmp/test -b sdc1-001hcnone.xls -s 1g
iozone -Ra -f /mnt/data2/tmp/test -b sdd1-001hcro.xls -s 1g
iozone -Ra -f /mnt/data3/tmp/test -b sde1-001hcrw.xls -s 1g
</code></p>

<p><a href="/files/2012_12/20121222-iozone-azurevm.zip">結果データ</a></p>

<h2>iozone の実行結果</h2>

<p>iozoneの測定結果をローカルドライブ、Data Diskの順で見ていく。それぞれの結果を図にした。</p>

<h3>ローカルディスクの性能</h3>

<p>まずは、ローカルドライブの実行結果から見る。読み込みはレコードサイズが8Kあたりから256KBまでは、2,500,000 KB/sec  - 3,000,000 KB/sec で、レコードサイズが増えていくとだんだん遅くなっていく。書き込み側は同じ軸ではとスケールが違い過ぎてよくわからない。</p>

<ul>
<li>図1 /dev/sdb2 ローカルドライブ 2012/12/22 測定
<img src="/images/2012_12/sdb2-rw.png" title="図1 /dev/sdb2 ローカルドライブ 2012/12/22 測定 " alt="図1 /dev/sdb2 ローカルドライブ 2012/12/22 測定" /></li>
</ul>


<p>そこで、書き込みの系統だけを表示させた。Record Rewriteの結果が桁外れに速い。これは<a href="http://www.iozone.org/docs/IOzone_msword_98.pdf" title="Iozone Filesystem Benchmark Download Documentation">ドキュメント</a>によると、同じ内容を繰り返し書き込むテストということなのでキャッシュの効果だろうと思われる。</p>

<ul>
<li>図1-1 /dev/sdb2 ローカルドライブ 書き込みのみ表示(1) 2012/12/22 測定
<img src="/images/2012_12/sdb2-note1-w.png" title="図1-1 /dev/sdb2 ローカルドライブ 書き込みのみ表示(1) 2012/12/22 測定" alt="図1-1 /dev/sdb2 ローカルドライブ 書き込みのみ表示(1) 2012/12/22 測定" /></li>
</ul>


<p>さらによく見ると、同じ再書き込みでも、Rewrite、Recoed Rewrite、Refwriteの違いがなかなか興味深い。Recoed Rewriteだけがリード並に桁外れに速い。Rewrite、Refwriteはファイル単位の再書き込みで、Recoed Rewriteは特定レコードの再書き込み（<a href="http://www.iozone.org/docs/IOzone_msword_98.pdf" title="Iozone Filesystem Benchmark Download Documentation">ドキュメント</a>から）ということなので、キャッシュが利く場合は限定されてるらしいことがわかる。
同じものを繰り返し書き込むというのは、現実にはあまり無いことなので、Rewriteをグラフから外して、書き込みのパフォーマンスを見やすくてみる。</p>

<ul>
<li>図2 /dev/sdb2 ローカルドライブ  書き込みのみ表示(2) 2012/12/22 測定
<img src="/images/2012_12/sdb2-w.png" title="図2 /dev/sdb2 ローカルドライブ  書き込みのみ表示(2) 2012/12/22 測定 " alt="図2 /dev/sdb2 ローカルドライブ  書き込みのみ表示(2) 2012/12/22 測定" /></li>
</ul>


<p>小さいブロックのランダム書き込みが苦手だということがわかる。これはHDDの一般的な傾向で納得できる。以降では書き込みの図は図2と同じデータ項目を表示する。</p>

<h3>Data Diskの性能</h3>

<p>話題のData Diskの性能に入る。ローカルドライ比較で、読み込みはほぼ同等な性能だったが、書き込みは半分程度の性能しか出ていない。
ホストキャッシュの設定で大きな違いが出ることを期待したが図を見る限りでは顕著な違いというほどには違いは無かった。</p>

<ul>
<li><p>図3 /dev/sdc1 ホストキャッシュなし 2012/12/22 測定
<img src="/images/2012_12/sdc1-rw.png" title="図3 /dev/sdc1 ホストキャッシュなし 2012/12/22 測定 " alt="図3 /dev/sdc1 ホストキャッシュなし 2012/12/22 測定 " /></p></li>
<li><p>図4 /dev/sdc1 ホストキャッシュなし 2012/12/22 書き込みのみ表示  測定
<img src="/images/2012_12/sdc1-w.png" title="図4 /dev/sdc1 ホストキャッシュなし 2012/12/22 書き込みのみ表示  測定" alt="図4 /dev/sdc1 ホストキャッシュなし 2012/12/22 書き込みのみ表示  測定 " /></p></li>
<li><p>図5 /dev/sdd1 ホストキャッシュ 読み取り専用 2012/12/22 測定
<img src="/images/2012_12/sdd1-rw.png" title="図5 /dev/sdd1 ホストキャッシュ 読み取り専用 2012/12/22 測定 " alt="図5 /dev/sdd1 ホストキャッシュ 読み取り専用 2012/12/22 測定 " /></p></li>
<li><p>図6 /dev/sdd1 ホストキャッシュ 読み取り専用 書き込みのみ表示 2012/12/22 測定
<img src="/images/2012_12/sdd1-w.png" title="図6 /dev/sdd1 ホストキャッシュ 読み取り専用 書き込みのみ表示 2012/12/22 測定 " alt="図6 /dev/sdd1 ホストキャッシュ 読み取り専用 書き込みのみ表示 2012/12/22 測定 " /></p></li>
<li><p>図7 /dev/sde1 ホストキャッシュ 読み取り/書き込み 2012/12/22 測定
<img src="/images/2012_12/sde1-rw.png" title="図7 /dev/sde1 ホストキャッシュ 読み取り/書き込み 2012/12/22 測定 " alt="図7 /dev/sde1 ホストキャッシュ 読み取り/書き込み 2012/12/22 測定 " /></p></li>
<li><p>図8 /dev/sde1 ホストキャッシュ 読み取り/書き込み 書き込みのみ表示 2012/12/22 測定
<img src="/images/2012_12/sde1-w.png" title=" 図8 /dev/sde1 ホストキャッシュ 読み取り/書き込み 書き込みのみ表示 2012/12/22 測定 " alt="図8 /dev/sde1 ホストキャッシュ 読み取り/書き込み 書き込みのみ表示 2012/12/22 測定 " /></p></li>
</ul>


<hr />

<h1>結論</h1>

<p>iozoneという選択肢がどうだったのかという気もしてるが、なかなか調子が良い。最初にパフォーマンス向上的な話ではなじめたが、7/4の結果に比べて劇的に変わっているという気はしない。
もう少しデータを精査する必要を感じるが、思ったより長くなりすぎたので、また別の方法を絡めて再考してみようと思う。</p>

<p>今回、テスト自体は一回しか走らせていないので再試験して結果を公開してもらえるとみんなの役に立つも思います。今まで、Azure Tableの性能評価をしたこともありましたが、何度か走らせると結果が違ったり、いつの間にかパフォーマンスが改善されたりなどすることがありました。
いろいろなパターンの性能情報があると設計が楽になり精度もあがりますし。</p>

<h1>Bookmarks</h1>

<ul>
<li><a href="http://www.iozone.org/" title="Iozone Filesystem Benchmark">Iozone Filesystem Benchmark</a></li>
<li><a href="http://www.iozone.org/docs/IOzone_msword_98.pdf" title="Iozone Filesystem Benchmark Download Documentation">Iozone Filesystem Benchmark Download Documentation</a></li>
<li><a href="http://sourceforge.jp/magazine/08/09/05/0759223" title="IOzoneによるファイルシステムのパフォーマンス測定">IOzoneによるファイルシステムのパフォーマンス測定</a></li>
<li><a href="http://www.slideshare.net/takekazuomi/azure-ubuntu-1204-iozone" title="Azure Ubuntu 12.04 iozone 速報 2012/7/4">Azure Ubuntu 12.04 iozone 速報 2012/7/4</a></li>
</ul>


<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Windows Azure Storage 2.0 の Blob Upload]]></title>
    <link href="http://takekazuomi.github.com/blog/2012/12/08/blobasyncinside/"/>
    <updated>2012-12-08T15:26:00+09:00</updated>
    <id>http://takekazuomi.github.com/blog/2012/12/08/blobasyncinside</id>
    <content type="html"><![CDATA[<p><a href="/blog/2012/12/08/WAAC2012Day2">前の記事</a>では非同期呼び出しを使っていますが、これには理由があります。
以前（2010年ぐらい）、Windows Azureを使い始めたころにSorage Client 1.xと、.NET Framework 4.0の組み合わせでいろいろ試した時には、スレッドを上げてやったのと非同期にしてやったので比べた時には有意な違いは出ませんでした。非同期でコードを書くと面倒になることも多かったので、「手間の割にはあまりメリットは無いなあ」というのが当時の結論でした。</p>

<p>ところが、2012年10月の末にAzure Storage Client 2.0が出てAPIや実装が大幅に変わったので変更点を眺めていたら面白いことに気が付きました。2.0ではBlobの書き込みは、Stream.WriteToSync()でやっていて、そのWriteToSyncの中が非同期呼び出しで実装されているとか、非同期呼び出し数をセマフォを使って制限しているところなどなかなか良さげな実装になっています。</p>

<p>ある日、<a href="https://github.com/chgeuer/AzureLargeFileUploader">AzureLargeFileUploader</a> というのがGitHubに上がっているのに気が付いて見てみたら、以前読んだ実装に比べて、そんなに優れているようには見えません。「あのコードより2.0の実装の方が大きなファイルでも効率的にUploadできるはず、もしかしたら2.0のコードは壊れているのかな？」と思い2.0のコードを動かして実際に試して見ました。やってみたらなかなか調子が良い、2.0の実装では十分な速度でBlobにアップロードされます。</p>

<p>C# 5.0で await/asyc もサポートされ .NET 4.5になってTask周りも改善されて非同期を使うには良い環境が揃ってきていると感じました。それで非同期呼び出しを使っているのです。</p>

<p><a href="http://www.flickr.com/photos/takekazuomi/8149221698/" title="shibuya by takekazu, on Flickr"><img src="http://farm9.staticflickr.com/8328/8149221698_e44be55a36_c.jpg" width="800" height="313" alt="shibuya"></a></p>

<hr />

<h1>試行（やってみた）</h1>

<p>Azure Datacenter内にLargeのインスタンスを用意して適当なファイルを元にして8GBのファイルを用意しました。そのファイルを同一のBlobに4回アップロードして平均の速度を測定します。結果は、<strong> 平均473Mbps </strong> でした。これは、ほぼインスタンスのネットワーク帯域制限値と同じです。なかなか良い結果と言えます。</p>

<p>確認に使ったコード、（メッセージがドイツ語になっているのは、AzureLargeFileUploader の名残です）</p>

<p><div><script src='https://gist.github.com/4133960.js?file='></script>
<noscript><pre><code>// #define OPENWRITE

using System;
using System.Configuration;
using System.IO;
using System.Net;
using Microsoft.WindowsAzure.Storage;
using Microsoft.WindowsAzure.Storage.Blob;


namespace UseUFS
{
    class Program
    {

        static void Main(string[] args)
        {
            ServicePointManager.DefaultConnectionLimit = 1024;

            if (args.Length != 1)
            {
                Console.Error.WriteLine(&quot;Bitte die Video-Datei zum Hochladen mit angeben...&quot;);
                return;
            }

            var filename = args[0];
            if (!File.Exists(filename))
            {
                Console.Error.WriteLine(&quot;Video-Datei \&quot;{0}\&quot; existiert nicht?&quot;, filename);
                return;
            }

            Console.WriteLine(&quot;Uploading {0}&quot;, filename);

            var connectionString = ConfigurationManager.AppSettings[&quot;storageaccount&quot;];
            Console.WriteLine(&quot;Using connection &quot; + connectionString);
            var storageAccount = CloudStorageAccount.Parse(connectionString);

            var containerName = ConfigurationManager.AppSettings[&quot;containername&quot;];

            upload(new FileInfo(filename), storageAccount, containerName);

        }

        private static void upload(FileInfo fileInfo, CloudStorageAccount storageAccount, string containerName)
        {
            var blobClient = storageAccount.CreateCloudBlobClient();
            var container = blobClient.GetContainerReference(containerName);
            container.CreateIfNotExists();

            var permission = container.GetPermissions();
            permission.PublicAccess = BlobContainerPublicAccessType.Container;
            container.SetPermissions(permission);

            //var blob = container.GetBlockBlobReference(fileInfo.Name);
            var blob = container.GetPageBlobReference(fileInfo.Name);


            blobClient.ParallelOperationThreadCount = Environment.ProcessorCount * 12;

#if OPENWRITE
            using (var stream = new FileStream(fileInfo.FullName, FileMode.Open, FileAccess.Read))
            using (var toStream = blob.OpenWrite())
            {
                stream.CopyToAsync(toStream).Wait();
            }
#else
            using (var stream = new FileStream(fileInfo.FullName, FileMode.Open, FileAccess.Read))
            {
                blob.UploadFromStream(stream);
            }

#endif

        }
    }
}






</code></pre></noscript></div>
</p>

<p>このコードのポイントは下記の3点です。</p>

<ol>
<li>18行目ので接続数の制限を1024に設定していること</li>
<li>59行目で並列度の設定をコア数の12倍にしていること</li>
<li>55,56行目ではPage/Block Blobのどちらを使うかを切り替えていること</li>
</ol>


<p>接続が作れないと並列度が上がらないのでDefaultConnectionLimitを増やし、Storage Client 2.0ではParallelOperationThreadCount のデフォルトが1になっているのでコア数の12倍に設定します。
Storage Client 2.0では、55, 56行目のように切り替えるだけで、どちらでも並列アップロードができるようになっています。1.xのときは、UploadFromStreamを使った時にBlock Blobでしか並列アップロードがサポートされてなかったことに比べて改善されています。</p>

<p>アップロード中をリソースマネージャーで観察するとコネクションが数多く作成されているのが確認できます。右側のNetworkトラフィックのグラフが波打っているのが興味深いところです。ピーク時に600-700Mbps程度行くこともありますが平均すると470 Mbpsという結果でした。CPUは5-10%程度しか使われていませんし、メモリーも開始から終了までほぼ一定です。なかなか優秀です。</p>

<p><img src="/images/2012-08-screen01.png" alt="Resource Monitor" /></p>

<hr />

<p>ここからは、ソースを見ながら確認していった過程のメモです。リンクばかりで分かり辛いかもしれませんが参考までに。興味深いのは非同期と同期の処理の境界と並列度の制限をしている部分です。</p>

<hr />

<h1>どうしてこんなところが変わったの？ ParallelOperationThreadCount のデフォルト値</h1>

<h2>1.x では</h2>

<p>CloudBlobClientに、ParallelOperationThreadCount というのがあります。１系では、下記のように定義されていました。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/sdk_1.7.1/microsoft-azure-api/StorageClient/CloudBlobClient.cs#L261">StorageClient/CloudBlobClient.cs#L261</a></p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/sdk_1.7.1/microsoft-azure-api/StorageClient/CloudBlobClient.cs#L52">CloudBlobClient.cs#L52</a></p>

<p>試しに、下記のようなコードでioThreadsを確認したところデスクトップPCでは２，Azure上のLargeのインスタンスでは４でした。どちらの環境でもデフォルトでParallelOperationThreadCountが２以上になり並列で動作します。</p>

<p><div><script src='https://gist.github.com/4239681.js?file='></script>
<noscript><pre><code>using System;

namespace ConsoleApplication10
{
    class Program
    {
        static void Main(string[] args)
        {
            int workerThreads;
            int ioThreads;

            System.Threading.ThreadPool.GetMinThreads(out workerThreads, out ioThreads);
            Console.WriteLine(&quot;workerThreads {0},  ioThreads {1}&quot;, workerThreads, ioThreads);
        }
    }
}
</code></pre></noscript></div>
</p>

<h2>2.0 では</h2>

<p>それに対し、２系では下記のように定義されています。parallelismFactorは、47行目付近で1で初期化されておりデフォルトは1となります。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/Common/Blob/CloudBlobClientBase.cs#L232">CloudBlobClientBase.cs#L232</a></p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/Common/Blob/CloudBlobClientBase.cs#L47">CloudBlobClientBase.cs#L47</a></p>

<p>これからParallelOperationThreadCount のデフォルトが1に変わったことがわかります。これは、<a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2012/10/29/windows-azure-storage-client-library-2-0-breaking-changes-amp-migration-guide.aspx">Windows Azure Storage Client Library 2.0 Breaking Changes &amp; Migration Guide</a> にも書いてあるBreaking Changesです。</p>

<p>2.0に移行した後、Block Blobのアップロードが遅くなった場合はParallelOperationThreadCountを確認するといいかもしれません。</p>

<hr />

<h1>ParallelOperationThreadCountの使われ方</h1>

<p>1.xでは、ParallelOperationThreadCount は、ParallelUpload で並列度を決めるために使われる。このクラスは、Streamをblock blobにUploadするもので、BlobClient.UploadFromStreamを、Block blobで使った時しか使われません。Page Blobでは並列アップロードは実装されていません。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/sdk_1.7.1/microsoft-azure-api/StorageClient/ParallelUpload.cs">ParallelUpload.cs</a></p>

<p>ParallelExecute あたりの処理をみると、Block毎にTaskを上げているらしいことがわかります。
<a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/sdk_1.7.1/microsoft-azure-api/StorageClient/ParallelUpload.cs#L148">ParallelUpload.cs#L148</a></p>

<p>2.0.1では、CloudBlockBlob のUploadFromStreamは、並列処理をするときにはStreamの拡張メソッドのWriteToSyncを呼んでいる。
<a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/DotNetCommon/Blob/CloudBlockBlob.cs#L116">CloudBlockBlob.cs#L116</a></p>

<hr />

<h1>同期と非同期の境界<a name="borderofsyncasync"></a></h1>

<p>WriteToSyncの実装は下記のようになっている。
<a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/Common/Core/Util/StreamExtensions.cs#L64">StreamExtensions.cs#L64</a></p>

<p>WriteToSyncは、読み込み側のStreamを非同期で読み出すためのフラグをもっているだけで書き込みは同期しているので、並列動作はせずに、ParallelOperationThreadCountに2以上をセットしてもパラレルアップロードは行われないように見えるが、 toStream.Write の実装を見ると内部が非同期に処理されているのがわかる。</p>

<p>toStreamの実態は、BlobをStreamとして扱うBlobWriteStreamのインスタンスで、これは内部的に非同期で書き込みを行う。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/DotNetCommon/Blob/BlobWriteStream.cs">BlobWriteStream.cs</a></p>

<p>呼び出し側を見ると同期処理のように見えるが、BlobWriteStreamBase で、AsyncSemaphore　parallelOperationSemaphoerをParallelOperationThreadCountの数で初期化しており、書き込みは非同期に行われる。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/DotNetCommon/Blob/BlobWriteStream.cs#L286">BlobWriteStream.cs#L286</a></p>

<hr />

<h1>非同期実行数の制限</h1>

<p>ここで、AsyncSemaphoreは、既定の数以上に処理が実行されないように非同期実行数を制御している役割を果たしている。</p>

<p><a href="https://github.com/WindowsAzure/azure-sdk-for-net/blob/master/microsoft-azure-api/Services/Storage/Lib/DotNetCommon/Core/Util/AsyncSemaphore.cs">AsyncSemaphore.cs</a></p>

<p>BlobWriteStreamでは、書き込みが全部終わると、最後に PutBlockList して終了する。同様な処理がPage Blobにも用意されていて並列アップロードされるような実装になっている。</p>

<p>このあたりは、<a href="http://msdn.microsoft.com/en-us/library/windowsazure/jj721952.aspx">What's New in Storage Client Library for .NET (version 2.0)</a>に書いてある説明通りの実装になってるようだ。</p>

<hr />

<h1>結論</h1>

<p>Blobのアップロードのような I/O がボトルネックとなるような処理ではI/O の非同期を使うことでCPU、メモリの負荷を最低限にして効率的に処理をすることができる。このコードでは、Stream 書き込みの内部処理を非同期化することで全体のパフォーマンスを向上しプログラミングモデルへの影響は最低限にしている。
サーバーサイドのプログラミングではこのような、同期、非同期の境界を発見して設計することが重要だと言える。非同期実行数の制限もなかなか興味深い。</p>

<hr />

<h1>おまけ</h1>

<p>LargeのRoleからStorageにUploadしたら450Mbps程度の速度が出た。ローカルからも、20Mbps程度だったので結構速い。転送中を見ていると、しばらくは複数のコネクションを使ってデータ転送していて最後にコネクションが一本になって終わる。、</p>

<p>PutBlobを非同期でやって最後にPutBlobListで終了となってるようだ。PutBlobの処理中はCPUはほとんど使われずに、ネットワーク帯域がボトルネックになっるぐらいには効率がいい。最後のPutBlobListの間はStorage側の待ちになってしまう。</p>

<p>これを考えると、複数のファイルをUploadする場合は、スレッドを分けて個々に処理した方が短時間で終わるのではないかと考えられる。ただ、あまり多くのスレッドを起動するメリットは無さそうだ。</p>

<p>今回は、UploadFromStreamを使ったが下記の説明にはOpenWriteを使うとStreamのように処理できると書いてある。やってみたら同じように動いた。つまりBlobをStreamとして使えるってことだ素晴らしい。</p>

<p><a href="http://msdn.microsoft.com/en-us/library/windowsazure/microsoft.windowsazure.storage.blob.cloudblockblob.openwrite.aspx">CloudBlockBlob.OpenWrite Method</a></p>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Azure Storage Gen 2は速かった]]></title>
    <link href="http://takekazuomi.github.com/blog/2012/12/08/WAAC2012Day2/"/>
    <updated>2012-12-08T11:11:00+09:00</updated>
    <id>http://takekazuomi.github.com/blog/2012/12/08/WAAC2012Day2</id>
    <content type="html"><![CDATA[<p>今年も早いもので、あっという間に12月になりました。個人的なAzure今年の目玉は、Azure Storageのパフォーマンスの向上(Gen2)と新しくなったWindows Azure Storage 2.0です。</p>

<p>IaaS、Web Site、Mobile Service、Media Serviceなど新機能満載なAzureですが、目立たないところで地味にストレージ関連は改善されています。ストレージはクラウドの足回りなので重要です。</p>

<ul>
<li>元記事は、Windows Azure Advent Calendar 2012 2日目として書きました。</li>
</ul>


<p><a href="http://atnd.org/events/34353">Windows Azure Advent Calendar jp: 2012</a></p>

<p><a href="http://www.flickr.com/photos/takekazuomi/8217239370/" title="omikuji by takekazu, on Flickr"><img src="http://farm9.staticflickr.com/8484/8217239370_f6ebb8d21d_z.jpg" width="640" height="480" alt="omikuji"></a></p>

<hr />

<h1>Azure Storageのパフォーマンスの向上</h1>

<p>2012/6/7 以降に作成されたストレージアカウントで、下記のようにパフォーマンスターゲットが引き上げられました。Gen 2と呼ばれているようです。以前のもの（Gen1）に比べ秒間のトランザクションベースだと4倍程度になっています（Azure Table 1Kエンティティの場合）</p>

<p>詳しくはリンク先を見てもらうとして下記の4点が注目です。</p>

<ol>
<li>ストレージ ノード 間のネットワーク速度が1Gbpsから10Gbpsに向上</li>
<li>ジャーナリングに使われるストレージデバイスがHDDからSSDに改善</li>
<li>単一パーテーション  500 エンティティ/秒 ->   2,000 エンティティ/秒 (15Mbps)</li>
<li>複数パーテーション 5,000 エンティティ/秒 -> 20,000 エンティティ/秒 (156Mbps)</li>
</ol>


<p>参照：<a href="http://satonaoki.wordpress.com/2012/11/03/windows-azure%E3%81%AE%E3%83%95%E3%83%A9%E3%83%83%E3%83%88-%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF-%E3%82%B9%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B8%E3%81%A82012%E5%B9%B4%E7%89%88%E3%82%B9/">Windows Azureのフラット ネットワーク ストレージと2012年版スケーラビリティ ターゲット</a></p>

<hr />

<h1>確認しよう</h1>

<p>ではどれだけ速くなったのか確認しましょう。なるべく実利用環境に近いようにということでC#を使います。ライブライは、最近出たばかりですが、Azure Storage Client 2.0を使います。このライブラリのコードをざっと見た感じだと、従来のコードに比べてシンプルになって読みやすく速度も期待できそうです。</p>

<p>比較的限界が低い単一パーテーションで確認します。前記のGen2の記事には、エンティティが1KByteで、単一パーテーションの場合、2,000 エンティティ/秒というパフォーマンスターゲットが記述されています。これを確認しようとするとAzure外部からのネットワークアクセスだと厳しいのでWorkerRoleを立てて、リモートデスクトップでログインしてプログラムを実行します。プログラムは秒間2000オブジェクトを計測時間の間は作りづけないといけないのでCPUやGCがボトルネックになるかもしれません、今回はLargeのインスタンスを使うことにしました。</p>

<p>Largeだとメモリ7GByte、coreが8つ、ネットワーク400Mbpsというスペックなので気にしなくても良いかと思ったのですが、GCをなるべく減らすためにエンティティのデータ部分をCache（共有）します。1KByteぐらいだとあまり効果が無いかもしれませんが。</p>

<p><div><script src='https://gist.github.com/4238298.js?file='></script>
<noscript><pre><code>public class EntityNk : TableEntity
{
  const int MAX_PROPERTY = 8; 
  private static List&lt;byte[]&gt; dataCache;
  private static int dataSize = 1;

  static EntityNk()
  {
    Clear();
  }

  public EntityNk(string partitionKey, string rowKey)
  {
    this.PartitionKey = partitionKey;
    this.RowKey = rowKey;
    this.Data0 = dataCache[0];
    this.Data1 = dataCache[1];
    this.Data2 = dataCache[2];
    this.Data3 = dataCache[3];
    this.Data4 = dataCache[4];
    this.Data5 = dataCache[5];
    this.Data6 = dataCache[6];
    this.Data7 = dataCache[7];
  }

  public EntityNk() { }

  public byte[] Data0 { get; set; }
  public byte[] Data1 { get; set; }
  public byte[] Data2 { get; set; }
  public byte[] Data3 { get; set; }
  public byte[] Data4 { get; set; }
  public byte[] Data5 { get; set; }
  public byte[] Data6 { get; set; }
  public byte[] Data7 { get; set; }

  public static int DataSize
  {
    set
      {
	if (value != dataSize)
	{
	    Clear();

	    dataSize = value;
	    var x = dataSize / MAX_PROPERTY;
	    var y = dataSize % MAX_PROPERTY;

	    for (var i = 0; i &lt; dataCache.Count(); i++)
	    {
		dataCache[i] = GetRandomByte(x);
	    }

	    if (y != 0)
	      dataCache[x] = GetRandomByte(y);
	  }
      }
  }
}
</code></pre></noscript></div>
</p>

<p>さらに、Threadを上げる数を減らして並列性を上げるために非同期呼び出しを使います。.NET 4.5 から await/async が使えるので割合簡単に非同期コードが記述できるのですが、少し手間がかかりました。</p>

<p>なんと残念ながら、Windows Azure Storage 2.0になっても APM (Asynchronous Programming Model) のメソッドしか用意されておらず、 await で使えるTaskAsyncの形式がサポートされていません。仕方がないので、自分で拡張メソッドを書きますが、引数が多くて intellisense があっても混乱します。泣く泣く、コンパイルエラーで期待されているシグニチャーをみながら書きました。コードとしてはこんな感じで簡単です。</p>

<p><div><script src='https://gist.github.com/4238639.js?file='></script>
<noscript><pre><code>public static class CloudTableExtensions
{
  public static Task&lt;TableResult&gt; ExecuteAsync(this CloudTable cloudTable, TableOperation operation, TableRequestOptions requestOptions = null, OperationContext operationContext = null, object state = null)
  {
    return Task.Factory.FromAsync&lt;TableOperation, TableRequestOptions, OperationContext, TableResult&gt;(
												      cloudTable.BeginExecute, cloudTable.EndExecute, operation, requestOptions, operationContext, state);
  }
}
</code></pre></noscript></div>
</p>

<p>この辺りは、下記のサイトが詳しくお勧めです。</p>

<p>参照：<a href="http://ufcpp.net/study/csharp/sp5_async.html#async">++C++; // 未確認飛行C 非同期処理</a></p>

<p><a name="pending"></a>このコードを動かしてみたら、「単一スレッド＋非同期の組み合わせだと、おおよそ２から３程度のコネクションしか作成されない」ことに気が付きました。場合によっては、5ぐらいまで上がることもあるようですが、どうしてこうなるのか不思議です。</p>

<h4><strong> これは、Azure Storage Client 2.0のBUG </strong> だったようです。2.0.2で修正されています。<a href="https://github.com/WindowsAzure/azure-sdk-for-net/pull/134">WindowsAzure/azure-sdk-for-net Issue #141:</a></h4>

<p><strong> <a href="/blog/2012/12/26/asc2-dot-0asyncbug/">2012/12/26 このFIXに関するまとめを書きました</a> </strong></p>

<p>非同期でガンガンリクエストが飛ぶのかと思ったのですが、それほどでもなかったので、今回のコードは複数スレッド（Task）をあげて、それぞれのスレッド内で非同期呼び出しを使って処理を行うようになっています。Taskの起動には、Parallel.ForEach を使っています。</p>

<p>さらに、上限に挑戦するためにEntity Group Transactionを使います。TableBatchOperation のインスタンスを作って操作を追加していってCloudTableのExecuteBatchAsync()で実行します。この辺りは以前の使い方とだいぶ違っています。
今回は時間を測っているだけですが、resultにはEntityのリストが帰ってきて、それぞれにtimestampとetagがセットされています。</p>

<p><div><script src='https://gist.github.com/4238661.js?file='></script>
<noscript><pre><code>         
var batchOperation = new TableBatchOperation();

foreach (var e in entityFactory(n))
{
    batchOperation.Insert(e);
}

var cresult = new CommandResult { Start = DateTime.UtcNow.Ticks };
var cbt = 0L;
var context = GetOperationContext((t) =&gt; cbt = t);
try
{
    var results = await table.ExecuteBatchAsync(batchOperation, operationContext: context);
    cresult.Elapsed = cbt;
}
catch (Exception ex)
{
    cresult.Elapsed = -1;
    Console.Error.WriteLine(&quot;Error DoInsert {0} {1}&quot;, n, ex.ToString());
}
return cresult;
</code></pre></noscript></div>
</p>

<hr />

<h1>結果</h1>

<p>いくつかパラメータを調整して実行し、スロットリングが起きる前後を探して4回測定しました。ピークe/sは、もっとも時間当たりのエンティティの挿入数が大きかった時の数字で秒間のエンティティ挿入数を表しています。
単一プロセスでスレッドを増やしていく方法では頭打ちになってしまうので、複数のプロセスを起動して測定ています。（このあたりも少しオカシイです）
下記の表の最初のカラムは起動するプロセス数です。</p>

<p>失敗が無かったケースで6,684、 6,932 エンティティ/秒で処理できており、Gen2で挙げられているパフォーマンスターゲットは十分達成できているようです。</p>

<p>測定時間の、Table Metricsを見るとThrottlingErrorと同時に、ClientTimeoutErrorも出ているのでプロセスを3つ上げているケースではクライアント側でサーバからの戻りが受けきれずにエラーになっている場合も含まれているようです。</p>

<table border="1" width="90%">
    <caption>表1 条件：エンティティサイズ 1KByte、単一パーテーション、スレッド数12、バッチサイズ100</caption>
    <thead>
      <tr>
        <th>プロセス数</th>
        <th>最少</th>
        <th>中央値</th>
        <th>平均</th>
        <th>最大</th>
        <th>90%点</th>
        <th>95%点</th>
        <th>99%点</th>
        <th>ピークe/s</th>
        <th>成功数</th>
        <th>失敗数</th>
      </tr>
    </thead>

    <tbody align="right">
      <tr>
        <th>2</th>
        <td>97.27</td>
        <td>166.6</td>
        <td>258.0</td>
        <td>14,800</td>
        <td>359.578</td>
        <td>472.373</td>
        <td>1,106.282</td>
        <td>6,684</td>
        <td>40,000</td>
        <td>0</td>
      </tr>

      <tr>
        <th>2</th>
        <td>94.17</td>
        <td>260.5</td>
        <td>333.7</td>
        <td>5,320</td>
        <td>564.774</td>
        <td>723.272</td>
        <td>1,339.027</td>
        <td>6,932</td>
        <td>40,000</td>
        <td>0</td>
      </tr>

      <tr>
        <th>3</th>
        <td>90.13</td>
        <td>174.8</td>
        <td>734.1</td>
        <td>21,270</td>
        <td>1,621.490</td>
        <td>1,845.903</td>
        <td>3,434.256</td>
        <td>7,218</td>
        <td>59,377</td>
        <td>623</td>
      </tr>

      <tr>
        <th>3</th>
        <td>90.35</td>
        <td>341.6</td>
        <td>610.1</td>
        <td>27,490</td>
        <td>1,064.593</td>
        <td>1,380.415</td>
        <td>4,431.789</td>
        <td>8,005</td>
        <td>59,740</td>
        <td>260</td>
      </tr>
    </tbody>
</table>


<hr />

<h1>最後に</h1>

<p>今回、第一世代（Gen 1）の単一パーテーションで500 エンティティ/秒というパフォーマンスターゲットに比べ10倍近いパフォーマンスを出しているのが測定できました。測定時間が短かったので、継続してこのパフォーマンスがでるのかどうかなど検証の余地はありますが、劇的に向上していると言えます。
<a href="https://github.com/takekazuomi/WAAC201202">takekazuomi/WAAC201202のレポジトリ</a>に計測に使ったコードをいれてあります。</p>

<p>12/2の担当でしたが、JSTでは日付も変わってだいぶ遅くなってしました。データの解析に最近お気に入りの（慣れない）「R」を使ったのですが、いろいろ手間取ってしまいました。最初はRで出した図なども入れたいと思ったのですが、軸や凡例の設定がうまくできずに時間切れで断念です。</p>

<p>レポジトリには、なんかずいぶん古い履歴まで上がってしましたが、手元のコードを使いまわしたら出てしまいました。スルーでお願いします。</p>

<hr />

<h1>おまけ</h1>

<p>数時間振り回してみると、エンティティ/秒の中央値は2000から3000エンティティ/秒程度になりそうです。負荷がかかり始めると、Gen １ではスロットリングをかけてエラーにしてしまうという動きでしたが、Gen 2 ではスロットリングを随時掛けつつ2000から3000エンティティ/秒程度に絞っていくという動きになったようです。</p>
]]></content>
  </entry>
  
</feed>
